{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting GOOGL Stock Closing Price on a Time Series (with PyQrack quantum associative memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"PyQrack\" is a pure Python language standard wrapper for the (C++11) Qrack quantum computer simulator library. PyQrack exposes a \"quantum neuron\" called \"`QrackNeuron`.\" (Its API reference is [here](https://pyqrack.readthedocs.io/en/latest/autoapi/pyqrack/qrack_neuron/index.html).) We'd like to model a simple data set to achieve a proof-of-concept of using `QrackNeuron`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyqrack in /home/iamu/qrack_venv/lib/python3.12/site-packages (1.77.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyqrack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model time-series data (ex.: stock closing price, climate data, biological rhythms, infrastructure load, etc.), we can start with OLS with point in time as predictor, to isolate any dominant \"straight-line\" trend over time. That hopefully explains a significant part of the variance. (ex.: \"The major industrial stock market indices tend to return ~7% APR, on average.\") Then, we **subtract the OLS prediction and new residual mean as baseline**. Then, we bin the data by training set quantile and apply the _**quantum Fourier transform (QFT)**_ to the time index. If we have time series data on a perfectly regular interval cadence, the QFT just produces _**uniform superposition**._ However, this is _uniform superposition_ that represents _low-frequency oscillation periods_ in the data. Then, `QrackNeuron` can _infer_ connections between _dependent variables_ and _frequency._ (ex.: \"Boom/bust cycles and deep market corrections tend to happen every 7-to-11 years, roughly,\" or, \"There is an element of semi-reliable _annual and quarterly seasonality_ to market sentiment and behavior.\") Then, we combine the inference model, OLS, and mean-value behavior back to together to get overall predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the data set into a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Date   Open   High    Low  Close   Volume Name  CompanyID\n",
      "0  732314  77.76  79.35  77.24  79.11  3117200  MMM         19\n",
      "1  732315  79.49  79.49  78.25  78.71  2558000  MMM         19\n",
      "2  732316  78.41  78.65  77.56  77.99  2529500  MMM         19\n",
      "3  732317  78.64  78.90  77.64  78.63  2479500  MMM         19\n",
      "4  732320  78.50  79.83  78.46  79.02  1845600  MMM         19\n",
      "Number of observations:  93612\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "all_data = pd.read_csv('stonks/all_stocks_2006-01-01_to_2018-01-01.csv')\n",
    "all_data['Date'] = pd.to_datetime(all_data['Date'], format='%Y-%m-%d').apply(lambda x:x.toordinal())\n",
    "all_data['CompanyID'] = all_data['Name'].astype('category').cat.codes\n",
    "\n",
    "print(all_data.head())\n",
    "print(\"Number of observations: \", all_data.shape[0])\n",
    "\n",
    "train, test = train_test_split(all_data.loc[all_data['Name'] == 'GOOGL'].dropna(), shuffle=False)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the dependent and independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Date']\n",
    "# dependents = ['Open', 'High', 'Low', 'Close', 'Volume' ]\n",
    "dependents = ['Close' ]\n",
    "\n",
    "X = train[features]\n",
    "y = train[dependents]\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[dependents]\n",
    "\n",
    "y_mean = y.mean()\n",
    "y -= y_mean\n",
    "y_test -= y_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a baseline, our first choice to model most data sets, at least in a _explorator_ capacity, might be linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:  Close\n",
      "Linear regression (OLS) validation R^2:  0.6959668592970805\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "regr = []\n",
    "for i in range(len(dependents)):\n",
    "    regr.append(linear_model.LinearRegression())\n",
    "    regr[i].fit(X, y[dependents[i]])\n",
    "    pd.DataFrame(zip(X.columns, regr[i].coef_))\n",
    "\n",
    "y_pred = [r.predict(X) for r in regr]\n",
    "y_proj = [r.predict(X_test) for r in regr]\n",
    "sst = [0.0] * len(regr)\n",
    "ssr = [0.0] * len(regr)\n",
    "for i in range(len(dependents)):\n",
    "    dependent = dependents[i]\n",
    "    for j in range(len(y_proj[i])):\n",
    "        sst[i] += y_test[dependent][j] * y_test[dependent][j]\n",
    "        ssr[i] += (y_test[dependent][j] - y_proj[i][j]) * (y_test[dependent][j] - y_proj[i][j])\n",
    "    print(\"Variable: \", dependents[i])\n",
    "    print(\"Linear regression (OLS) validation R^2: \", 1 - ssr[i] / sst[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we'd like to make an improvement on the goodness-of-fit of linear regression by combining it with PyQrack's `QrackNeuron`. To start, to first order, we want to eliminate the \"straight-line\" component of time dependence, from linear regession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = y.copy()\n",
    "yp_test = y_test.copy()\n",
    "\n",
    "for i in range(len(dependents)):\n",
    "    if (1 - ssr[i] / sst[i]) <= 0:\n",
    "        continue\n",
    "\n",
    "    dependent = dependents[i]\n",
    "    for j in range(len(y_pred[i])):\n",
    "        yp.at[j, dependent] -= y_pred[i][j]\n",
    "    for j in range(len(y_proj[i])):\n",
    "        yp_test.at[j, dependent] -= y_proj[i][j]\n",
    "\n",
    "yp_mean = yp.mean()\n",
    "\n",
    "for i in range(len(dependents)):\n",
    "    if (1 - ssr[i] / sst[i]) <= 0:\n",
    "        yp_mean[dependents[i]] = 0\n",
    "\n",
    "yp -= yp_mean\n",
    "yp_test -= yp_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class can only work with discrete, binary data. To model this or any data set, we have to reduce it to a simple, discrete, binary form.\n",
    "\n",
    "We'll try to model the data set via \"(quantum) associative memory.\" There are several statistical considerations, to avoid overfit.\n",
    "\n",
    "Firstly, each possible discretized independent variable permutation input trains an independent parameter of a `QrackNeuron`. If a `QrackNeuron` has never seen a specific, exact permutation of input bits, it has no information about them at all, so its prediction defaults to \"maximal superposition,\" (i.e. a totally random guess). Therefore, we'd like to keep our number of possible distinct inputs significantly fewer in number than our observation rows, when we discretize our indepedent variables.\n",
    "\n",
    "Satisfying the first consideration, we secondly discretize our dependent variable to have exactly as many possible discrete values as possible distinct inputs. (We guess that this loses the least information about the dependent variable, while we still have enough observations to fully train our network.)\n",
    "\n",
    "Thirdly, our learning rate should should just barely \"saturate\" the learned parameters of our (quantum) associative memory. As a learning volatility parameter (\"`eta`\") of `1/2` \"fully trains\" one parameter of a `QrackNeuron` between input qubits and output qubit, on average, this implies that we might set `eta` to `1/2` times `2` to the power of input qubits (summed across all predictors) divided by the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_qubit_counts = [5]\n",
    "out_qubit_counts = [5]\n",
    "\n",
    "in_tot_qubits = sum(in_qubit_counts)\n",
    "in_bin_counts = [(1 << i) for i in in_qubit_counts]\n",
    "in_tot_bins = sum(in_bin_counts)\n",
    "in_qubits = list(range(in_tot_qubits))\n",
    "out_tot_qubits = sum(out_qubit_counts)\n",
    "out_bin_counts = [(1 << o) for o in out_qubit_counts]\n",
    "out_tot_bins = sum(out_bin_counts)\n",
    "out_qubits = list(range(in_tot_qubits, in_tot_qubits + out_tot_qubits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To discretize the data, we split it into as many quantiles as `2` to the power of our number of input qubits. For date or time data, we'll introduce a separate parameter to control choice of quantiles, and we'll transform to the frequency domain. Fitting to frequency rather than point in time, we potentially capture periodic correlations in weather, as opposed to non-periodic changes with monotonically increasing time.\n",
    "\n",
    "Once we have our quantiles, we bin our indepedent training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyqrack import QrackSimulator, QrackNeuron\n",
    "\n",
    "xd = []\n",
    "yd = []\n",
    "xd_test = []\n",
    "yd_test = []\n",
    "y_bins = []\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    l = list(X[feature])\n",
    "    xd_bounds_col = QrackNeuron.quantile_bounds(l, in_qubit_counts[i])\n",
    "    xd.append(QrackNeuron.discretize(l, xd_bounds_col))\n",
    "    xd_test.append(QrackNeuron.discretize(list(X_test[feature]), xd_bounds_col))\n",
    "\n",
    "for i, dependent in enumerate(dependents):\n",
    "    l = list(yp[dependent])\n",
    "    yd_bounds_col_2 = QrackNeuron.quantile_bounds(l, out_qubit_counts[i] + 1)\n",
    "    yd_bounds_col = yd_bounds_col_2[0::2]\n",
    "    yd.append(QrackNeuron.discretize(l, yd_bounds_col))\n",
    "    yd_test.append(QrackNeuron.discretize(list(yp_test[dependent]), yd_bounds_col))\n",
    "    y_bins.append(yd_bounds_col_2[1::2])\n",
    "\n",
    "xd = QrackNeuron.flatten_and_transpose(xd)\n",
    "xd_test = QrackNeuron.flatten_and_transpose(xd_test)\n",
    "yd = QrackNeuron.flatten_and_transpose(yd)\n",
    "yd_test = QrackNeuron.flatten_and_transpose(yd_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference model (with QFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is based on a very simple assumption: the dependent variable values can be inferred from their relationship to both _**(linear) time**_ and _**periodic oscillation frequency**._ As such, the `Date` column is transformed via an _inverse quantum Fourier transform (QFT)_ before training or prediction, to capture dependence on _oscillation frequency._ (The _linear_ vs. _oscillatory_ dimensions are assumed to be approximately or exactly _orthogonal._)\n",
    "\n",
    "(It's time to train our inference model!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2264  out of  2264\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "eta = (1 / 2) * (sum(in_bin_counts) / y.shape[0])\n",
    "input_indices = list(range(in_tot_qubits))\n",
    "qsim = QrackSimulator(in_tot_qubits + out_tot_qubits)\n",
    "\n",
    "qft_qubits = list(range(in_qubit_counts[0]))\n",
    "\n",
    "output_layer = []\n",
    "for i in range(out_tot_qubits):\n",
    "    output_layer.append(QrackNeuron(qsim, input_indices, in_tot_qubits + i))\n",
    "\n",
    "# Train the network to associate powers of 2 with their log2()\n",
    "print(\"Learning...\")\n",
    "for i in range(len(xd)):\n",
    "    clear_output(wait=True)\n",
    "    print(\"Epoch \", (i + 1), \" out of \", len(xd))\n",
    "    \n",
    "    perm = xd[i]\n",
    "    res = yd[i]\n",
    "\n",
    "    for j in range(out_tot_qubits):\n",
    "        qsim.reset_all()\n",
    "        for k in range(in_tot_qubits):\n",
    "            if perm[k]:\n",
    "                qsim.x(k)\n",
    "        # Transform time domain to Fourier basis\n",
    "        qsim.qft(qft_qubits)\n",
    "        output_layer[j].learn(eta, res[j] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With training complete, we predict the validation set and calculate the coefficient of determination (R^2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting  755  out of  755\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Should associate each input with its trained output...\")\n",
    "dependents_len = len(dependents)\n",
    "sum_sqr_tot = [0.0] * dependents_len\n",
    "sum_sqr_res = [0.0] * dependents_len\n",
    "sum_sqr_tot_p = [0.0] * dependents_len\n",
    "sum_sqr_res_p = [0.0] * dependents_len\n",
    "shots = min(10 ** 6, 1 << (out_tot_qubits + 2))\n",
    "for i in range(len(xd_test)):\n",
    "    clear_output(wait=True)\n",
    "    print(\"Predicting \", (i + 1), \" out of \", len(xd_test))\n",
    "    \n",
    "    perm = xd_test[i]\n",
    "\n",
    "    qsim.reset_all()\n",
    "    for j in range(in_tot_qubits):\n",
    "        if perm[j]:\n",
    "            qsim.x(j)\n",
    "    # Transform time domain to Fourier basis\n",
    "    qsim.qft(qft_qubits)\n",
    "\n",
    "    for j in range(out_tot_qubits):\n",
    "        output_layer[j].predict()\n",
    "\n",
    "    m_res = dict(Counter(qsim.measure_shots(out_qubits, shots)))\n",
    "\n",
    "    front = 0\n",
    "    for j in range(dependents_len):\n",
    "        pred = 0\n",
    "        mid_mask = out_bin_counts[j] - 1\n",
    "        for k, v in m_res.items():\n",
    "            pred += y_bins[j][(k >> front) & mid_mask] * v / shots\n",
    "        front += out_qubit_counts[j]\n",
    "\n",
    "        dependent = dependents[j]\n",
    "        \n",
    "        sum_sqr_tot_p[j] += yp_test[dependent][i] * yp_test[dependent][i]\n",
    "        sum_sqr_res_p[j] += (yp_test[dependent][i] - pred) * (yp_test[dependent][i] - pred)\n",
    "\n",
    "        if (1 - ssr[j] / sst[j]) > 0:\n",
    "            pred += (yp_mean[dependent] + y_proj[j][i])\n",
    "\n",
    "        sum_sqr_tot[j] += y_test[dependent][i] * y_test[dependent][i]\n",
    "        sum_sqr_res[j] += (y_test[dependent][i] - pred) * (y_test[dependent][i] - pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to the validation R^2 of linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:  Close\n",
      "Linear regression (OLS) validation R^2:  0.6959668592970805\n",
      "QrackNeuron (periodic-only) validation R^2:  0.6103271807000468\n",
      "QrackNeuron + OLS validation R^2:  0.881526548901667\n",
      "QrackNeuron validation MSR:  27831.44888794739\n",
      "QrackNeuron validation RMSE:  166.8276022963448\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dependents)):\n",
    "    dependent = dependents[i]\n",
    "    print(\"Variable: \", dependent)\n",
    "    print(\"Linear regression (OLS) validation R^2: \", 1 - ssr[i] / sst[i])\n",
    "    print(\"QrackNeuron (periodic-only) validation R^2: \", 1 - sum_sqr_res_p[i] / sum_sqr_tot_p[i])\n",
    "    print(\"QrackNeuron + OLS validation R^2: \", 1 - sum_sqr_res[i] / sum_sqr_tot[i])\n",
    "    msr = sum_sqr_res[i] / y_test[dependent].shape[0]\n",
    "    print(\"QrackNeuron validation MSR: \", msr)\n",
    "    print(\"QrackNeuron validation RMSE: \", math.sqrt(msr))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8388,
     "sourceId": 11883,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
